{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NSMC_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5BNIXJe0NKi"
      },
      "source": [
        "<**데이터 load**>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKgG_muVyDL1"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z-r9cMy1Ari",
        "outputId": "c3537e2a-1e8e-4b54-ae64-8d2d7091e044"
      },
      "source": [
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nsmc'...\n",
            "remote: Enumerating objects: 14763, done.\u001b[K\n",
            "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
            "Receiving objects: 100% (14763/14763), 56.19 MiB | 17.31 MiB/s, done.\n",
            "Resolving deltas: 100% (1749/1749), done.\n",
            "Checking out files: 100% (14737/14737), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TXlwpeZyRdf"
      },
      "source": [
        "train_data = pd.read_table(\"nsmc/ratings_train.txt\")\n",
        "test_data = pd.read_table(\"nsmc/ratings_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4wiY1H00n9L",
        "outputId": "8768e9cc-023d-4ce8-c191-d6f35cea0a65"
      },
      "source": [
        "# 데이터(리뷰) 개수 확인\n",
        "print('train data 개수: ', len(train_data))\n",
        "print('test data 개수: ', len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data 개수:  150000\n",
            "test data 개수:  50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "QPnUkgag1Kzg",
        "outputId": "f5146cef-6770-4fb8-f3b5-d72dcb39c561"
      },
      "source": [
        "# train data 형태 확인\n",
        "\n",
        "train_data[:5] # 상위 5개 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxcgxvbM394Q"
      },
      "source": [
        "[Columns of the dataset]\n",
        "\n",
        "학습 데이터에서 상위 5개의 리뷰를 출력해봄으로써, 데이터의 형태를 확인할 수 있다. NSMC 데이터셋은 3개의 feature, 혹은 column으로 이뤄져있는데 각가 id, document, label이다. \n",
        "- id: 각 리뷰에 부여한 인식표임으로 감정 분석(분류)에서는 유용한 column이 아니다. \n",
        "- document: 리뷰 내용에 해당하며, 데이터 셋 정보에 따르면 각 리뷰는 140자 미만이다. \"...\" 등의 불용어를 제거해야할 필요성을 확인할 수 있다.\n",
        "- label: 해당 데이터는 labeled 데이터임을 알 수 있다. 긍정의 리뷰의 경우 1, 부정의 리뷰의 경우 0의 값이 각 데이터 당 부여되어 있다.\n",
        "\n",
        "=> 결국, NSMC 데이터셋을 이용한 감정 분석 프로젝트는 document와 label 2개의 열을 학습하는 모델을 구축하는 것임을 알 수 있다.\n",
        "\n",
        "[Information from the dataset]\n",
        "\n",
        "- 한국어 데이터의 특성: 띄어쓰기가 제대로 되어 있지 않은 데이터가 존재한다.\n",
        "=> 처리 필요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "tF6jdqih5dWu",
        "outputId": "2fda0b23-cac3-439b-d18d-2c21f8c6fb65"
      },
      "source": [
        "# test data 형태 확인\n",
        "\n",
        "test_data[:5] # 상위 5개 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6270596</td>\n",
              "      <td>굳 ㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9274899</td>\n",
              "      <td>GDNTOPCLASSINTHECLUB</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8544678</td>\n",
              "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6825595</td>\n",
              "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6723715</td>\n",
              "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                           document  label\n",
              "0  6270596                                                굳 ㅋ      1\n",
              "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
              "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
              "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
              "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b6tS--d6Xbh"
      },
      "source": [
        "Test 데이터도 train 데이터와 마찬가지의 3개의 열로 이뤄져있으며, 리뷰에 해당하는 document 역시 전처리가 필요함을 확인할 수 있다. 이 전처리는, train 데이터에 대한 전처리 과정을 동일하게 적용시키면 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bebXM2Hz0ZKb"
      },
      "source": [
        "<**데이터 정제/전처리**>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb-dyya51WVO",
        "outputId": "21512e72-fa03-4206-df31-fd2918198b48"
      },
      "source": [
        "# train data의 중복 확인하기\n",
        "train_data['document'].nunique(), train_data['label'].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(146182, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6dJeTrZK2pP"
      },
      "source": [
        "학습 데이터의 중복을 확인하는 과정에서, document 즉, 리뷰에서 중복을 제거한 데이터의 개수가 약 14만 6천개라는 것은 약 4천개의 데이터가 중복되어 있다는 의미이다. label 열에 대해서는 0과 1만 존재하므로 2라는 값이 출력된 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDGpCGLT1wU9"
      },
      "source": [
        "# train data의 중복 제거하기 - document 열에서 중복 내용 제거\n",
        "train_data.drop_duplicates(subset=['document'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBFRBYmJL3vx",
        "outputId": "316ce981-7646-4a71-fc57-07567e3c9cb2"
      },
      "source": [
        "# train data의 중복 확인\n",
        "print('학습 데이터의 총 샘플의 개수: ', len(train_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 데이터의 총 샘플의 개수:  146183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "IXECUoL4Mpn-",
        "outputId": "91c1df84-1e60-4cf0-eebf-5048f23a6e53"
      },
      "source": [
        "# 중복을 제거한 train data의 긍정/부정 label 값의 분포 확인\n",
        "train_data['label'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f02d4afcba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyv87zLIOshU",
        "outputId": "1f0f99dc-d654-4e6f-be15-da3c985986ce"
      },
      "source": [
        "# 중복을 제거한 train data의 긍정/부정 lable 값의 정확한 개수 확인\n",
        "print(train_data.groupby('label').size().reset_index(name = 'count'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label  count\n",
            "0      0  73342\n",
            "1      1  72841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXVB1VGTOc1p"
      },
      "source": [
        "[학습 데이터의 중복 documents를 제거한 후, label의 분포]\n",
        "\n",
        "약 146000개의 학습 데이터에서 긍정과 부정의 label의 분포가 균일해보이고, 정확한 개수를 파악해본 결과 label=0의 데이터가 근소하게 많은 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JigWHlYOZsG",
        "outputId": "553a1b46-2904-49cb-ffb2-280c31ef237b"
      },
      "source": [
        "# 학습 데이터 중 NULL 값 존재 여부 확인하기\n",
        "print(train_data.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id          0\n",
            "document    1\n",
            "label       0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKaTa9kLQiru"
      },
      "source": [
        "[학습 데이터에서, NULL 값 확인 결과]\n",
        "\n",
        "id와 label은 NULL인 데이터가 없지만, document에 NULL인 데이터가 하나 존재한다는 것을 확인하였다. 본 프로젝트는 document와 label을 학습하는 것인데, document에 NULL 값이 존재하면 학습할 데이터가 없는 것이므로, NULL이 포함된 해당 데이터를 삭제하겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VamUn8SMQgCi",
        "outputId": "53283bdf-13e9-486f-e2d0-e7978417abe0"
      },
      "source": [
        "# 학습 데이터에서 NULL 포함 데이터 행 삭제\n",
        "train_data = train_data.dropna(how='any') # 삭제\n",
        "print(train_data.isnull().values.any()) # 삭제 후, NULL 값이 존재하는 지 확인"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RMDf9JMS9Oq"
      },
      "source": [
        "- 현재까지 NSMC 데이터의 분포를 확인해보고, 학습 데이터를 전처리하게 전에 NULL 값이 포함된 데이터를 제거하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjJzHRIuS2Zd"
      },
      "source": [
        "# 위에서 데이터를 출력해봄으로써, 학습과 관련 없는 특수 문자 및 기호를 제거한다\n",
        "# -> 한글 텍스트만 남김\n",
        "# 정규 표현식을 활용하여 한글의 범위 지정: 자음의 범위(ㄱ-ㅎ), 모음의 범위(ㅏ-ㅣ)\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\" \")\n",
        "# 한글과 공백을 제외하고 모두 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "RlTQRH-Qlnq5",
        "outputId": "8b174a4a-22a6-4b9f-ec33-a7f96f707da4"
      },
      "source": [
        "train_data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙   진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼   솔직히 재미는 없다  평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙   진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼   솔직히 재미는 없다  평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzveqYBUltr3"
      },
      "source": [
        "- 전처리 전에 출력한 데이터와 비교해서 기존의 공백과 띄워쓰기는 유지되면서 구두점과 기호는 제거되었음을 확인할 수 있다.\n",
        "- 여기서 또 다른 전처리의 필요성이 대두된다!\n",
        "  * 왜냐하면, 네이버 영화 리뷰는 특수 기호나 영어 등 한글과 공백 이외의 문자만으로도 작성이 가능한데 이런 document들은 위의 전처리를 통해 NULL 값이 되었을 것이기 때문이다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbF1PHmvlolV"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhKWPJxJm5l-",
        "outputId": "70e46b3f-5dbe-4d88-f33e-330796798ee6"
      },
      "source": [
        "# 한글과 공백만 남기는 전처리 이후 NULL 값의 존재 여부 확인\n",
        "train_data['document'].replace('', np.nan, inplace=True)\n",
        "print(train_data.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id          0\n",
            "document    0\n",
            "label       0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M_vpED6nEXh"
      },
      "source": [
        "- 첫번째 전처리 후, NULL 값으로 바뀐 document 데이터가 391개나 생겼음을 알 수 있다. document feature를 통해서 학습을 해하고, label과 다르게 평균 혹은 수동 imputing이 불가능함으로 의미 없는 데이터로 판정하고 제거한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cYT_rEBnB0m",
        "outputId": "555f37db-322e-4cf9-ba53-17df456ecded"
      },
      "source": [
        "# NULL 값을 포함한 학습 데이터 삭제\n",
        "train_data = train_data.dropna(how = 'any')\n",
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "146182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnxZbhHinnx1"
      },
      "source": [
        "* 지금까지 학습 데이터에 수행한\n",
        "\n",
        "  - document 열에서의 중복 제거\n",
        "  - 한글 범위와 공백에 벗어나는 데이터 제거\n",
        "  - NULL 값 제거\n",
        "\n",
        "  => 3 가지의 일련의 전처리 과정을 테스트 데이터에도 동일하게 진행한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHBnL0Y5nfaE"
      },
      "source": [
        "# document 열에서 중복 제거\n",
        "test_data.drop_duplicates(subset = ['document'], inplace=True)\n",
        "# 정규 표현식으로 한글 범위와 공백 이외의 문자 제거\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "# 공백을 NULL 값으로 변경 후 NULL 값 제거\n",
        "test_data['document'].replace('', np.nan, inplace=True)\n",
        "test_data = test_data.dropna(how='any')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO9y1eXdoXsF",
        "outputId": "fcdca347-c923-40eb-c5b5-1498f79a849b"
      },
      "source": [
        "# 전처리 후 테스트 데이터의 개수 확인\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전처리 후 테스트용 샘플의 개수 : 48995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "armK-y-5okSt"
      },
      "source": [
        "**<토큰화>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ5oQQD9o0Ok"
      },
      "source": [
        "- 토큰화 과정에서는 불용어 제거를 첫번째 목표로 한다.\n",
        "- 불용어는 조사와 접속사를 우선 의미하며, 지속적으로 데이터를 검토하면서 처리할 계획이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRVZ5Qf9ocZ2"
      },
      "source": [
        "# 불용어 정의\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olm0mzIx4Mor"
      },
      "source": [
        ">> 불용어 정의를 더 확장해볼 수 있을 것 같다!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hn3xY9CpWVT",
        "outputId": "6a685f12-fa35-4141-96c3-1fdc793ca0da"
      },
      "source": [
        "# 토큰화를 위한 형태소 분석기로 KoNLPy의 Okt를 사용하기로 한다\n",
        "# 한국어 토큰화는 영어처럼 띄어쓰기 기준을 적용하지 않고 형태소 분석기를 적용한다\n",
        "!pip3 install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 9.7MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 49.3MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.4)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, beautifulsoup4, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dFxLM5Mp3ZR"
      },
      "source": [
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVYFKhC-4ZJZ"
      },
      "source": [
        ">> 형태소 분석기 간의 성능 차이를 살펴봐도 좋을 것 같다!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pR97t5XAqhZQ"
      },
      "source": [
        "okt = Okt() # konlpy에서 제공하는 형태소 분석기\n",
        "# train_data에 형태소 분석기를 사용하여 토큰화를 하면서 불용어를 제거하여 X_train에 저장한다\n",
        "X_train = []\n",
        "for sentence in train_data['document']:\n",
        "    temp_X = []\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화, stemp=True는 일정 수준의 정규화를 수행해준다\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(temp_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk_ELTHTq8sD"
      },
      "source": [
        "# 상위 3개의 데이터만 출력하여 결과 확인하기\n",
        "print(X_train[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0wBHROrtHGq"
      },
      "source": [
        "- 토큰화를 진행하고 불용어를 제거함으로써, 어느 정도의 정규화가 이뤄진 형태소로 tokenize 된 것을 볼 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om64TSXAtDwr"
      },
      "source": [
        "# test 데이터에 대해서도 동일하게 토큰화 진행\n",
        "X_test = []\n",
        "for sentence in test_data['document']:\n",
        "    temp_X = []\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(temp_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKnOXUvFuJei"
      },
      "source": [
        "* 지금까지 train_data와 test_data에 대해서 텍스트 전처리를 수행해 보았다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ7K3cstuQVl"
      },
      "source": [
        "**<정수 인코딩>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omjRJ0jfuUeE"
      },
      "source": [
        "- machine이 한글 텍스트를 숫자로 처리하여 학습할 수 있도록, \n",
        "훈련 데이터와 테스트 데이터를 정수로 인코딩을 해야한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ6C4JTctZME"
      },
      "source": [
        "# train 데이터에 대해서 단어 집합 만들기\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w80TqPSyuyBU"
      },
      "source": [
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7d18dhyv0Uk"
      },
      "source": [
        "# 단어 집합이 생성되고, 각 단어에 고유한 정수가 부여된 것을 확인하기\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2_-9nAcwEpJ"
      },
      "source": [
        "- 단어가 4만 3천여개가 넘개 존재하는데, 이때 각 단어에 부여된 정수는 전체 train 데이터에서 빈도수가 높은 순서대로 부여되었다.\n",
        "- 빈도수를 반영하기 위해서, 빈도수가 낮은 단어들이 이 데이터에서 차지하는 비중을 확인하고 자연어 처리에서 배제하고자 한다.\n",
        "* 여기서의 가정: 빈도수가 매우 낮은 단어들은 감정 분석에 중요한 학습 요소가 아니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULzXkDJxv8c9"
      },
      "source": [
        "# 빈도수 하한선을 3회로 잡고, 빈도수가 3회 미만인 단어들의 비중 확인해보기\n",
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR6KnDOqxUZd"
      },
      "source": [
        "# 단어와 빈도수 쌍을 key와 value로 받기\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ1YlP09xslP"
      },
      "source": [
        "* 위의 분석을 통해서, 등장 빈도가 3회 미만인 단어들에 대한 통계를 확인할 수 있다.\n",
        "  - 등장 빈도가 3회 미만인 단어들을 이하 희귀 단어로 칭한다.\n",
        "  - 희귀 단어들이 단어 집합에서 차지하는 비중은 절반 이상이다.\n",
        "  - 하지만, 실제로 train 데이터에서 등장 빈도로 차지하는 비중은 1.87% 정도로 매우 적다.\n",
        "\n",
        "  => 따라서, 위의 희귀 단어들을 정수 인코딩 과정에서 배제시키고, 희귀 단어들을 제외한 단어 개수를 단어 집합의 최대 크기로 제한한다.\n",
        "\n",
        "  >> 빈도수 threshold를 변화시켜가면서 포함할 단어의 범위를 조정해볼 수 있을 것 같다!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgYYEOBVxoTY"
      },
      "source": [
        "# 전체 단어 개수 중 빈도수 3 미만인 단어 개수는 제거\n",
        "# 0번 패딩 토큰을 고려하여 +1한다\n",
        "vocab_size = total_cnt - rare_cnt + 1 \n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImO9HxAjyyTF"
      },
      "source": [
        "# Keras의 tokenizer에 단어 집합의 크기를 인자로 넘겨주기\n",
        "# -> 텍스트 sequence를 정수 sequence로 변환하는 인코딩 과정에서\n",
        "#    인자보다 큰 숫자가 부여된 단어들은 제외시킨다\n",
        "tokenizer = Tokenizer(vocab_size) \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XsWDekDzQQ-"
      },
      "source": [
        "# 정수 인코딩의 결과 확인을 위해 3개의 데이터 출력해보기\n",
        "print(X_train[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YadOP6AOzcGu"
      },
      "source": [
        "- 각 데이터의 단어들이 해당하는 정수로 변환된 것을 볼 수 있다.\n",
        "* 단어의 개수는 19416개로 인자를 통해 제한되었으므로, 0번 단어에서부터 19415번 단어까지만 사용함을 의미한다.\n",
        "  - 0번 단어는 패딩을 위한 토큰이다.\n",
        "  - 19416 이상의 숫자는 train 데이터에 더이상 존재하지 않음을 의미한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ri4fPStzaLC"
      },
      "source": [
        "# train_data 별도 저장\n",
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG-r97fK0tXv"
      },
      "source": [
        "**<empty 샘플 제거>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uUAcQGS0yrp"
      },
      "source": [
        "* 전체 데이터에서 빈도수가 낮은 희귀 단어를 삭제했다\n",
        "* -> 이는 빈도수가 낮은 단어들로만 구성된 샘픋들이 empty해졌다는 것을 의미한다!\n",
        "* empty 샘플들은 label이 어떤 값이든 학습에 사용할 수 없는 의미 없는 데이터이므로 제거해주는 작업을 거쳐야한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV0I7VOcz-HJ"
      },
      "source": [
        "# 각 샘플들의 길이를 확인해서, 길이가 0인 empty 샘플들의 인덱스 받아오기\n",
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "drop_test = [index for index, sentence in enumerate(X_test) if len(sentence) < 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k0B4KMT1N2a"
      },
      "source": [
        "# drop_train에는 X_train의 empty 샘플들의 인덱스가 저장됨\n",
        "# drop_test에는 X_test의 empty 샘플들의 인덱스가 저장됨\n",
        "# empty 샘플 제거\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\n",
        "print(len(X_train))\n",
        "print(len(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyTpgere1jzB"
      },
      "source": [
        "- 이전의 train 데이터 (X_train과 y_train_)의 데이터 개수는 145791개 였는데, empty 샘플의 제거를 통해서 샘플의 개수가 줄어든 것을 확은할 수 있다.\n",
        "- test 데이터에 대해서도 빈 값을 동일한 과정을 통해 제거해준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH3_5YZw1eDZ"
      },
      "source": [
        "# test 데이터의 empty 샘플 제거하기\n",
        "X_test = np.delete(X_test, drop_test, axis=0)\n",
        "y_test = np.delete(y_test, drop_test, axis=0)\n",
        "print(len(X_test))\n",
        "print(len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1KqMcrk2Ozh"
      },
      "source": [
        "<**패딩**>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szXvffyA2TW9"
      },
      "source": [
        "- 서로 길이가 다른 샘플들의 길이를 동일하게 맞춰주는 작업: 패딩 작업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faPU8gFT14XJ"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "# 전체 데이터에서 가장 길이가 긴 리뷰와 전체 데이터의 길이 분포 파악하기\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(s) for s in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixj5IGkB2tu8"
      },
      "source": [
        "- 리뷰의 최대 길이는 69이다\n",
        "- 전체 데이터의 길이 분포는 평균 길이 값인 10 내외이다.\n",
        "* 모델이 처리할 수 있도록 X_train과 X_test의 모든 샘플 길이를 동일하게 맞춰줘야한다\n",
        "* -> 특정 길이를 정해야한다!\n",
        "  - 특정 길이 변수를 max_len으로 정하고 대부분의 리뷰가 잘리지 않도록 하는 최적의 변수 값을 찾아낸다.\n",
        "    -  전체 샘플들 중에서 길이가 max_len 이하인 샘플의 비율을 확인함으로써 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny9k9m0V2p96"
      },
      "source": [
        "# 전체 샘플 중에서 길이가 max_len 이하인 샘플의 비율을 확인하는 함수\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgR9IyNf3URP"
      },
      "source": [
        "# 전체 길이 분포 그래프에서 max_len = 30을 상정하여, 비율을 확인하기\n",
        "max_len = 30\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVDvLS2p3rDs"
      },
      "source": [
        "- 함수를 통해서 전체 train 데이터 중 약 94%의 리뷰가 30 이하의 길이를 가지는 것을 확인했다\n",
        "- 모든 샘플의 길이를 30으로 맞추겠다.\n",
        "\n",
        ">> max_len을 조정하면서 패딩에 변화를 주고 결과 변화를 볼 수 있을 것 같다!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h54X5Aj63nU_"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSu1DASq43qL"
      },
      "source": [
        "**<LSTM으로 감성 분류>**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOreomuX36iI"
      },
      "source": [
        "# 모델 구축을 위해 필요한 도구 import\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUkTs4Pm5BjU"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-z9PkJm5I53"
      },
      "source": [
        ">임베딩 벡터의 차원을 100으로 정했는데, 임베딩 벡터의 차원 공부하기!\n",
        "\n",
        "> LSTM(128)에서 128 인자의 역할 공부하기!\n",
        "\n",
        ">> activation 함수를 바꿔보는 것이 어떤 영향을 줄 지 공부하기!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLUBQDA05VwI"
      },
      "source": [
        "* 학습 과정에서 검증 데이터 손실 (val_loss)가 다시 증가하면, overfit의 징후이므로 검증 데이터 손실이 4회 증가하면 학습을 조기 종료하여 효율적인 학습을 시도해았다.\n",
        "* ModelCheckpoint를 활용해서 검증 데이터의 정확도 (val_acc)가 이전보다 좋아질 경우에만 모델을 저장한다\n",
        "\n",
        ">> val_loss 증가 인자를 조정해볼 수 있다!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4h3nGr75IN0"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNg7BlYW5v03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6d1dcc-3bb9-4af3-da7c-20c14332e5b9"
      },
      "source": [
        "# 에포크를 총 15번 수행함\n",
        "# train 데이터 중 20%를 검증 데이터로 사용하면서 정확도를 확인함\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1936/1936 [==============================] - 31s 12ms/step - loss: 0.4388 - acc: 0.7913 - val_loss: 0.3527 - val_acc: 0.8445\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.84454, saving model to best_model.h5\n",
            "Epoch 2/15\n",
            "1936/1936 [==============================] - 22s 11ms/step - loss: 0.3308 - acc: 0.8564 - val_loss: 0.3342 - val_acc: 0.8554\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.84454 to 0.85542, saving model to best_model.h5\n",
            "Epoch 3/15\n",
            "1936/1936 [==============================] - 22s 11ms/step - loss: 0.3027 - acc: 0.8706 - val_loss: 0.3290 - val_acc: 0.8593\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.85542 to 0.85928, saving model to best_model.h5\n",
            "Epoch 4/15\n",
            "1936/1936 [==============================] - 22s 11ms/step - loss: 0.2813 - acc: 0.8829 - val_loss: 0.3273 - val_acc: 0.8586\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.85928\n",
            "Epoch 5/15\n",
            "1936/1936 [==============================] - 22s 11ms/step - loss: 0.2611 - acc: 0.8932 - val_loss: 0.3285 - val_acc: 0.8583\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.85928\n",
            "Epoch 6/15\n",
            "1936/1936 [==============================] - 23s 12ms/step - loss: 0.2509 - acc: 0.8985 - val_loss: 0.3336 - val_acc: 0.8567\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.85928\n",
            "Epoch 7/15\n",
            "1936/1936 [==============================] - 22s 12ms/step - loss: 0.2317 - acc: 0.9073 - val_loss: 0.3382 - val_acc: 0.8559\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.85928\n",
            "Epoch 8/15\n",
            "1936/1936 [==============================] - 22s 11ms/step - loss: 0.2153 - acc: 0.9147 - val_loss: 0.3494 - val_acc: 0.8527\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.85928\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVAde4Hv56-W"
      },
      "source": [
        ">> epoch나 검증 데이터 비율을 조정해본다!!!\n",
        "\n",
        "- 조기 종료 조건에 따라서 7번째 epoch에서 학습이 종료되었다.\n",
        "- test 데이터로 정확도를 측정할 때는, 위의 학습 과정에서 검증 데이터의 정확도가 가장 높았을 때 저장된 모델인 best_model.h5를 사용한다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD9AqYXq55vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "105ea949-8bef-43cc-cb2e-1572a1b9c0ba"
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1524/1524 [==============================] - 5s 3ms/step - loss: 0.3369 - acc: 0.8562\n",
            "1524/1524 [==============================] - 5s 3ms/step - loss: 0.3369 - acc: 0.8562\n",
            "\n",
            " 테스트 정확도: 0.8562\n",
            "\n",
            " 테스트 정확도: 0.8562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyJd5_qupXaq"
      },
      "source": [
        "<소스 코드 참고>\n",
        "https://wikidocs.net/44249\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbAq3IFWT6EG"
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    }
  ]
}